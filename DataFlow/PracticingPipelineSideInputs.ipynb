{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PracticingPipelineSideInputs\n",
    "\n",
    "**Assign the Dataflow Developer Role**\n",
    "If the account does not have the Dataflow Developer role, follow the steps below to assign the required role.\n",
    "•\tOn the Navigation menu, click IAM & Admin > IAM.\n",
    "•\tSelect the default compute Service Account {project-number}-compute@developer.gserviceaccount.com.\n",
    "•\tSelect the Edit option (the pencil on the far right).\n",
    "•\tClick Add Another Role\n",
    "•\tClick inside the box for Select a Role. In the Type to filter selector, type and choose Dataflow Developer.\n",
    "•\tClick Save.\n",
    "\n",
    "![dataflowDeveloperRole](Media/dataflowDeveloperRole.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✓ Open the SSH terminal and connect to the training VM <br>\n",
    "Compute Engine > VM instances > training-vm > Connect <br>\n",
    "✓ In the training-vm SSH terminal (Clone the training github repository) <br>\n",
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst <br>\n",
    "✓ Create a Cloud Storage bucket <br>\n",
    "Cloud Storage > Browser > Create Bucket <br>\n",
    "Name :<your unique bucket name (Project ID)> <br>\n",
    "Location type : Multi-Region <br>\n",
    "Location : <Your location> <br>\n",
    "✓ In training-vm SSH terminal init bucket , project variable  <br>\n",
    "BUCKET=\"<your unique bucket name (Project ID)>\" <br>\n",
    "echo $BUCKET <br>\n",
    "PROJECT=\"<your unique project name (Project ID)>\" <br>\n",
    "echo $PROJECT <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "✓ First query \n",
    "SELECT\n",
    "content\n",
    "FROM\n",
    "`fh-bigquery.github_extracts.contents_java_2016`\n",
    "LIMIT\n",
    "10\n",
    "✓ Second query \n",
    "SELECT\n",
    "COUNT(*)\n",
    "FROM\n",
    "`fh-bigquery.github_extracts.contents_java_2016`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✓ In VM Terminal\n",
    "cd ~/training-data-analyst/courses/data_analysis/lab2/python\n",
    "nano JavaProjectsThatNeedHelp.py\n",
    "Ctrl+X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import datetime, os\n",
    "import apache_beam as beam\n",
    "import math\n",
    "\n",
    "'''\n",
    "This is a dataflow pipeline that demonstrates Python use of side inputs. The pipeline finds Java packages\n",
    "on Github that are (a) popular and (b) need help. Popularity is use of the package in a lot of other\n",
    "projects, and is determined by counting the number of times the package appears in import statements.\n",
    "Needing help is determined by counting the number of times the package contains the words FIXME or TODO\n",
    "in its source.\n",
    "@author tomstern\n",
    "based on original work by vlakshmanan\n",
    "python JavaProjectsThatNeedHelp.py --project <PROJECT> --bucket <BUCKET> --DirectRunner or --DataFlowRunner\n",
    "'''\n",
    "\n",
    "# Global values\n",
    "TOPN=1000\n",
    "\n",
    "\n",
    "# ### Functions used for both main and side inputs\n",
    "\n",
    "def splitPackageName(packageName):\n",
    "   \"\"\"e.g. given com.example.appname.library.widgetname\n",
    "           returns com\n",
    "\t           com.example\n",
    "                   com.example.appname\n",
    "      etc.\n",
    "   \"\"\"\n",
    "   result = []\n",
    "   end = packageName.find('.')\n",
    "   while end > 0:\n",
    "      result.append(packageName[0:end])\n",
    "      end = packageName.find('.', end+1)\n",
    "   result.append(packageName)\n",
    "   return result\n",
    "\n",
    "def getPackages(line, keyword):\n",
    "   start = line.find(keyword) + len(keyword)\n",
    "   end = line.find(';', start)\n",
    "   if start < end:\n",
    "      packageName = line[start:end].strip()\n",
    "      return splitPackageName(packageName)\n",
    "   return []\n",
    "\n",
    "def packageUse(record, keyword):\n",
    "   if record is not None:\n",
    "     lines=record.split('\\n')\n",
    "     for line in lines:\n",
    "       if line.startswith(keyword):\n",
    "         packages = getPackages(line, keyword)\n",
    "         for p in packages:\n",
    "           yield (p, 1)\n",
    "\n",
    "def is_popular(pcoll):\n",
    " return (pcoll\n",
    "    | 'PackageUse' >> beam.FlatMap(lambda rowdict: packageUse(rowdict['content'], 'import'))\n",
    "    | 'TotalUse' >> beam.CombinePerKey(sum)\n",
    "    | 'Top_NNN' >> beam.transforms.combiners.Top.Of(TOPN, key=lambda kv: kv[1]) )\n",
    "\n",
    "\n",
    "\n",
    "def packageHelp(record, keyword):\n",
    "   count=0\n",
    "   package_name=''\n",
    "   if record is not None:\n",
    "     lines=record.split('\\n')\n",
    "     for line in lines:\n",
    "       if line.startswith(keyword):\n",
    "         package_name=line\n",
    "       if 'FIXME' in line or 'TODO' in line:\n",
    "         count+=1\n",
    "     packages = (getPackages(package_name, keyword) )\n",
    "     for p in packages:\n",
    "         yield (p,count)\n",
    "\n",
    "def needs_help(pcoll):\n",
    " return (pcoll\n",
    "    | 'PackageHelp' >> beam.FlatMap(lambda rowdict: packageHelp(rowdict['content'], 'package'))\n",
    "    | 'TotalHelp' >> beam.CombinePerKey(sum)\n",
    "    | 'DropZero' >> beam.Filter(lambda packages: packages[1]>0 ) )\n",
    "\n",
    "\n",
    "# Calculate the final composite score\n",
    "#\n",
    "#    For each package that is popular\n",
    "#    If the package is in the needs help dictionary, retrieve the popularity count\n",
    "#    Multiply to get compositescore\n",
    "#      - Using log() because these measures are subject to tournament effects\n",
    "#\n",
    "\n",
    "def compositeScore(popular, help):\n",
    "    for element in popular:\n",
    "      if help.get(element[0]):\n",
    "         composite = math.log(help.get(element[0])) * math.log(element[1])\n",
    "         if composite > 0:\n",
    "           yield (element[0], composite)\n",
    "\n",
    "\n",
    "# ### main\n",
    "\n",
    "# Define pipeline runner (lazy execution)\n",
    "def run():\n",
    "\n",
    "  # Command line arguments\n",
    "  parser = argparse.ArgumentParser(description='Demonstrate side inputs')\n",
    "  parser.add_argument('--bucket', required=True, help='Specify Cloud Storage bucket for output')\n",
    "  parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "  group = parser.add_mutually_exclusive_group(required=True)\n",
    "  group.add_argument('--DirectRunner',action='store_true')\n",
    "  group.add_argument('--DataFlowRunner',action='store_true')\n",
    "\n",
    "  opts = parser.parse_args()\n",
    "\n",
    "  if opts.DirectRunner:\n",
    "    runner='DirectRunner'\n",
    "  if opts.DataFlowRunner:\n",
    "    runner='DataFlowRunner'\n",
    "\n",
    "  bucket = opts.bucket\n",
    "  project = opts.project\n",
    "\n",
    "  #    Limit records if running local, or full data if running on the cloud\n",
    "  limit_records=''\n",
    "  if runner == 'DirectRunner':\n",
    "     limit_records='LIMIT 3000'\n",
    "  get_java_query='SELECT content FROM [cloud-training-demos:github_repos.contents_java] {0}'.format(limit_records)\n",
    "\n",
    "  argv = [\n",
    "    '--project={0}'.format(project),\n",
    "    '--job_name=javahelpjob',\n",
    "    '--save_main_session',\n",
    "    '--staging_location=gs://{0}/staging/'.format(bucket),\n",
    "    '--temp_location=gs://{0}/staging/'.format(bucket),\n",
    "    '--runner={0}'.format(runner),\n",
    "    '--region=us-central1',\n",
    "    '--max_num_workers=5'\n",
    "    ]\n",
    "\n",
    "  p = beam.Pipeline(argv=argv)\n",
    "\n",
    "\n",
    "  # Read the table rows into a PCollection (a Python Dictionary)\n",
    "  bigqcollection = p | 'ReadFromBQ' >> beam.io.Read(beam.io.BigQuerySource(project=project,query=get_java_query))\n",
    "\n",
    "  popular_packages = is_popular(bigqcollection) # main input\n",
    "\n",
    "  help_packages = needs_help(bigqcollection) # side input\n",
    "\n",
    "  # Use side inputs to view the help_packages as a dictionary\n",
    "  results = popular_packages | 'Scores' >> beam.FlatMap(lambda element, the_dict: compositeScore(element,the_dict), beam.pvalue.AsDict(help_packages))\n",
    "\n",
    "  # Write out the composite scores and packages to an unsharded csv file\n",
    "  output_results = 'gs://{0}/javahelp/Results'.format(bucket)\n",
    "  results | 'WriteToStorage' >> beam.io.WriteToText(output_results,file_name_suffix='.csv',shard_name_template='')\n",
    "\n",
    "  # Run the pipeline (all operations are deferred until run() is called).\n",
    "\n",
    "\n",
    "  if runner == 'DataFlowRunner':\n",
    "     p.run()\n",
    "  else:\n",
    "     p.run().wait_until_finish()\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  run()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline_graph](Media/pipeline_graph.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute the pipeline\n",
    "1.The program requires BUCKET and PROJECT values and choosing whether to run the pipeline locally using --\n",
    "DirectRunner or on the cloud using --DataFlowRunner <br>\n",
    "2.Execute the pipeline locally by typing the following into the training-vm SSH terminal. <br>\n",
    "python3 JavaProjectsThatNeedHelp.py --bucket $BUCKET --project  $PROJECT –DirectRunner <br>\n",
    "3. Cloud Storage > Browser > javahelp folder > Result <br>\n",
    "4.Execute the pipeline on the cloud by typing the following into the training-vm SSH terminal. <br>\n",
    "python3 JavaProjectsThatNeedHelp.py --bucket $BUCKET --project $PROJECT –DataFlowRunner <br>\n",
    "5. Monitor job in Dataflow <br>\n",
    "6. Cloud Storage > Browser > javahelp folder > Result <br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
