{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Simple Dataflow Pipeline\n",
    "\n",
    "✓ Check project permissions, Enable Dataflow API (see appendix) <br>\n",
    "✓ Open the SSH terminal and connect to the training VM <br>\n",
    "Compute Engine > VM instances > training-vm > Connect <br>\n",
    "✓ In training-vm SSH terminal Download Code Repository <br>\n",
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst <br>\n",
    "✓ Create a Cloud Storage bucket <br>\n",
    "Cloud Storage > Browser > Create Bucket  <br>\n",
    "Name :<your unique bucket name (Project ID)> <br>\n",
    "Location type : Multi-Region <br>\n",
    "Location : <Your location> <br>\n",
    "✓ In training-vm SSH terminal init bucket variable  <br>\n",
    "BUCKET=\"<your unique bucket name (Project ID)>\" <br>\n",
    "echo $BUCKET <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✓ In training-vm SSH terminal change directory and show code source and than Press Ctrl+X to exit Nano. <br>\n",
    "cd ~/training-data-analyst/courses/data_analysis/lab2/python  <br>\n",
    "nano grep.py <br>\n",
    "✓ Can you answer these questions about the file grep.py? <br>\n",
    "•What files are being read? <br>\n",
    "•What is the search term? <br>\n",
    "•Where does the output go? <br>\n",
    "There are three transforms in the pipeline: <br>\n",
    "•What does the transform do? <br>\n",
    "•What does the second transform do? <br>\n",
    "•Where does its input come from? <br>\n",
    "•What does it do with this input? <br>\n",
    "•What does it write to its output? <br>\n",
    "•Where does the output go to? <br>\n",
    "•What does the third transform do? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import sys\n",
    "\n",
    "def my_grep(line, term):\n",
    "   if line.startswith(term):\n",
    "      yield line\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   p = beam.Pipeline(argv=sys.argv)\n",
    "   input = '../javahelp/src/main/java/com/google/cloud/training/dataanalyst/javahelp/*.java'\n",
    "   output_prefix = '/tmp/output'\n",
    "   searchTerm = 'import'\n",
    "\n",
    "   # find all lines that contain the searchTerm\n",
    "   (p\n",
    "      | 'GetJava' >> beam.io.ReadFromText(input)\n",
    "      | 'Grep' >> beam.FlatMap(lambda line: my_grep(line, searchTerm) )\n",
    "      | 'write' >> beam.io.WriteToText(output_prefix)\n",
    "   )\n",
    "\n",
    "   p.run().wait_until_finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.In the training-vm SSH terminal, locally execute grep.py. <br>\n",
    "python3 grep.py <br>\n",
    "The output file will be output.txt. If the output is large enough, it will be sharded into separate \n",
    "parts with names like: output-00000-of-00001. <br>\n",
    "2.Locate the correct file by examining the file's time. <br>\n",
    "ls -al /tmp <br>\n",
    "3.Examine the output file(s). <br>\n",
    "4.You can replace \"-*\" below with the appropriate suffix. <br>\n",
    "cat /tmp/output-* <br>\n",
    "Does the output seem logical? <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the pipeline on the cloud\n",
    "\n",
    "\n",
    "1.Copy some Java files to the cloud. In the training-vm SSH terminal, enter the following commmand: <br>\n",
    "```shell\n",
    "gsutil cp ../javahelp/src/main/java/com/google/cloud/training/dataanalyst/javahelp/*.java \n",
    "gs://$BUCKET/javahelp\n",
    "```\n",
    "2.Using Nano, edit the Dataflow pipeline in grepc.py. <br>\n",
    "```shell\n",
    "nano grepc.py\n",
    "```\n",
    "3.Replace PROJECT and BUCKET with your Project ID and Bucket name. <br>\n",
    "```shell\n",
    "PROJECT='qwiklabs-gcp-your-value’ BUCKET='qwiklabs-gcp-your-value'\n",
    "qwiklabs-gcp-04-4491a9a7c668\n",
    "```\n",
    "Save the file and close Nano by pressing the CTRL+X key, then press Y, and Enter. <br>\n",
    "4.Submit the Dataflow job to the cloud: <br>\n",
    "```shell\n",
    "python3 grepc.py\n",
    "```\n",
    "Note: You may ignore the message: WARNING:root:Make sure that locally built Python SDK docker image has \n",
    "Python 3.7 interpreter. Your Dataflow job will start successfully.Because this is such a small job, running on the \n",
    "cloud will take significantly longer than running it locally (on the order of 7-10 minutes).\n",
    "5. Monitor job <br>\n",
    "6. Cloud Storage > Browser > javahelp folder > output.txt <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def my_grep(line, term):\n",
    "   if line.startswith(term):\n",
    "      yield line\n",
    "\n",
    "PROJECT='cloud-training-demos'\n",
    "BUCKET='cloud-training-demos'\n",
    "\n",
    "def run():\n",
    "   argv = [\n",
    "      '--project={0}'.format(PROJECT),\n",
    "      '--job_name=examplejob2',\n",
    "      '--save_main_session',\n",
    "      '--staging_location=gs://{0}/staging/'.format(BUCKET),\n",
    "      '--temp_location=gs://{0}/staging/'.format(BUCKET),\n",
    "      '--region=us-central1',\n",
    "      '--runner=DataflowRunner'\n",
    "   ]\n",
    "\n",
    "   p = beam.Pipeline(argv=argv)\n",
    "   input = 'gs://{0}/javahelp/*.java'.format(BUCKET)\n",
    "   output_prefix = 'gs://{0}/javahelp/output'.format(BUCKET)\n",
    "   searchTerm = 'import'\n",
    "\n",
    "   # find all lines that contain the searchTerm\n",
    "   (p\n",
    "      | 'GetJava' >> beam.io.ReadFromText(input)\n",
    "      | 'Grep' >> beam.FlatMap(lambda line: my_grep(line, searchTerm) )\n",
    "      | 'write' >> beam.io.WriteToText(output_prefix)\n",
    "   )\n",
    "\n",
    "   p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor job in dataflow\n",
    "![monitor](Media/monitor.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check permissions (IAM)\n",
    "![checkPermissions](./Media/checkPermissions.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the account is not present in IAM or does not have the editor role, follow the steps below to assign the required role.\n",
    "•\tIn the Google Cloud console, on the Navigation menu, click Home.\n",
    "•\tCopy the project number (e.g. 729328892908).\n",
    "•\tOn the Navigation menu, click IAM & Admin > IAM.\n",
    "•\tAt the top of the IAM page, click Add.\n",
    "•\tFor New principals, type:\n",
    "{project-number}-compute@developer.gserviceaccount.com \n",
    "Replace {project-number} with your project number.\n",
    "•\tFor Role, select Project (or Basic) > Editor. Click Save.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
