{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataflow : dataflow is a programming model that is designed to process large volumes of data in a *parallel* and scalable manner. It is a simple yet powerful model that allows developers to design flexible parallel pipelines for data transformation, across a variety of execution environments. Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes, including Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Dataflow pipelines can also be run on local machines or on resource managers like Apache Mesos and Apache YARN.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| critira | Dataflow | Dataproc |\n",
    "| ------- | -------- | -------- |\n",
    "| recommended for| new data processing pipelines,unified batch and streaming | existing Hadoop/Spark applications,ML and data science ecosystem,large_batch job,preemptibe VMs |\n",
    "|Fully managed | yes | No |\n",
    "|Auto scaling | yes,transform-by-transform (adaptive) | yes,based on cluster utilization(reactive) |\n",
    "|expertise| Apache Beam | hadoop,hive,pig,spark,sparkML,sparkR,presto,zeppelin,... |\n",
    "|serverless | yes | no |\n",
    "\n",
    "![pipeline](Media/pipeline.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![constructpipeline](Media/constructpipeline.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![constructBranchingPipeline](Media/constructBranchingPipeline.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pipeline is a directed graph of steps**  \n",
    "\n",
    "    p = beam.Pipeline()\n",
    "    p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "if --name-- == '--main--':\n",
    "    with beam.Pipeline(argv=sys.argv) as p :\n",
    "        (p\n",
    "         |beam.io.ReadFromText('gs://dataflow-samples/shakespeare/kinglear.txt') #read input\n",
    "         |beam.FlatMap(count_words) #find all words\n",
    "         |beam.CombinePerKey(sum) #combine words\n",
    "         |beam.io.WriteToText('gs://my-bucket/counts.txt') #write output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pipeline\n",
    "import apache_beam as beam\n",
    "options = {\n",
    "    'project': 'my-project', # replace with your project ID\n",
    "    'runner': 'DataflowRunner', # run on Cloud Dataflow where to run the pipeline\n",
    "    'staging_location': 'gs://my-bucket/staging',\n",
    "    'region': 'us-central1', # run in the Dataflow region closest to your location\n",
    "    'setup_file': './setup.py', # replace with your setup.py file\n",
    "    'temp_location': 'gs://my-bucket/temp',\n",
    "    'job_name': 'unique-job-name',\n",
    "}\n",
    "pipeline_options = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "pipeline = beam.Pipeline(options=pipeline_options) # create pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline execution using dataflow runner\n",
    "\n",
    "run local \n",
    "```shell\n",
    "python ./main.py\n",
    "```\n",
    "run on dataflow on cloud\n",
    "```shell\n",
    "python ./main.py \\\n",
    "--runner DataflowRunner \\\n",
    "--project $PROJECT_ID  \\\n",
    "--job_name $JOB_NAME  \\\n",
    "--temp_location gs://$BUCKET/tmp/  \\\n",
    "--staging_location gs://$BUCKET/staging/  \\\n",
    "--region $REGION\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data from local file system ,cloud storage,pub/sub,bigquery\n",
    "```python\n",
    "with beam.Pipeline(options=pipeline_options) as p:\n",
    "```\n",
    "read from cloud storage (return string)\n",
    "```python\n",
    "lines = p | beam.io.ReadFromText('gs://BUCKET_NAME/FILE_NAME')\n",
    "```\n",
    "read from pub/sub (return string)\n",
    "```python\n",
    "lines = p | beam.io.ReadFromPubSub(topic='projects/PROJECT_ID/topics/TOPIC_NAME')\n",
    "```\n",
    "read from bigquery (return rows)\n",
    "```python\n",
    "  query = 'SELECT * FROM `PROJECT_ID.DATASET.TABLE`'\n",
    "  bq_source = beam.io.BigQuerySource(query=query, use_standard_sql=True) #setup\n",
    "  bq-data = p | beam.io.Read(bq_source) #read\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **write to a bigquery table **\n",
    " establish reference to bigquery table\n",
    "```python\n",
    "table_spec = bigquery.TableReference(\n",
    "    projectId=PROJECT_ID,\n",
    "    datasetId=DATASET_ID,\n",
    "    tableId=TABLE_ID)\n",
    "```\n",
    "write to bigquery table\n",
    "```python\n",
    "bq_data | beam.io.WriteToBigQuery(\n",
    "    table_spec,\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create A PColletion from in-memory data\n",
    "```python\n",
    "city_zip_list = [\n",
    "    ('berlin', '10115'),\n",
    "    ('berlin', '10999'),\n",
    "    ('flensburg', '24937'),\n",
    "    ('flensburg', '24944'),\n",
    "    ('bremen', '28195'),\n",
    "    ('bremen', '28759'),\n",
    "    ('hamburg', '20095'),\n",
    "    ('hamburg', '20148'),\n",
    "]\n",
    "citycodes = p | 'CreateCityCodes' >> beam.Create(city_zip_list) # display name of pipeline step\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map and FlatMap\n",
    "```python\n",
    "# use map 1:1 transformation between input and output\n",
    "'wordLength' >> beam.Map(lambda word: len(word))\n",
    "# use flatmap 1:n transformation between input and output\n",
    "def my_grep(line, term):\n",
    "    if term in line:\n",
    "        yield line # yield is like return but for generator\n",
    "'grep' >> beam.FlatMap(lambda line: my_grep(line, 'cat')) # searchterm is 'cat'\n",
    "```\n",
    "flatmap is similar to map but it can return multiple values for each input element"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parDo implement parallel processing\n",
    "```python\n",
    "words = .... # input PCollection of strings\n",
    "class computeWordlengthFn(beam.DoFn): # DoFn is a class that contains the logic for processing elements of a PCollection\n",
    "    def process(self, element):\n",
    "        return [len(element)]\n",
    "wordlengths = words | beam.ParDo(computeWordlengthFn()) # output is PCollection of integers\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GroupByKey explicitly shuffles key-values pairs**\n",
    "```python\n",
    "cityAndZipcodes = p | beam. Map (fields[0], fields[1])\n",
    "grouped = cityAndZipCodes | beam. GroupByKey()\n",
    "```\n",
    "|cityandzip codes | grouped|\n",
    "|-----------------|--------|\n",
    "|Lexington, 40513 <br> Nashville, 37027 <br> Lexington, 40502 <br>Seattle, 98125 <br>Mountain View, 94041<br>Seattle, 98133<br>Lexington, 40591<br>Mountain View, 94085<br>| Lexington, [40513, 40502, 40592] <br>Nashville,[37027]<br>Seattle, [98125, 98133]<br> Mountain View, [94041, 94085] |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data skew makes grouping less efficient at scale\n",
    "\n",
    "![CoGroupByKey](Media/CoGroupByKey.png)\n",
    "\n",
    "combine(reduce) a pcollection\n",
    "```python\n",
    "# Applied to a PCollection of valies\n",
    "totalamount =salesAmounts | beam.CombineGlobally(sum)\n",
    "# Applied to a PCollection of key-value pairs\n",
    "totalsalesPerPerson = salesRecords | beam.CombinePerKey(sum)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**combinrFn works by overriding existing operations **\n",
    "```python\n",
    "class AverageFn(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return (0.0, 0) # sum, count\n",
    "    def add_input(self, sum_count, input):\n",
    "        (sum, count) = sum_count\n",
    "        return sum + input, count + 1\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        sums, counts = zip(*accumulators)\n",
    "        return sum(sums), sum(counts)\n",
    "    def extract_output(self, sum_count):\n",
    "        (sum, count) = sum_count\n",
    "        return sum / count if count else float('NaN')\n",
    "pc = p | beam.Create([1, 2, 3, 4, 5])\n",
    "average = pc | beam.CombineGlobally(AverageFn())\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![combine](Media/combine.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flatten](Media/flatten.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![partition](Media/partition.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sideinput](Media/sideinputs.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How side inputs work \n",
    "words = -\n",
    "```python\n",
    "words = p | beam.Create(['cat', 'dog', 'elephant', 'rat', 'rat', 'cat'])\n",
    "def filter_using_length(word,lower_bound,upper_bound=float('inf')):\n",
    "    if lower_bound <= len(word) <= upper_bound:\n",
    "        yield word\n",
    "small_words = words | 'small' >> beam.FlatMap(filter_using_length, 0,3)\n",
    "\n",
    "avg_word_length = (words | 'length' >> beam.Map(len) | 'average' >> beam.CombineGlobally(beam.combiners.MeanCombineFn()))\n",
    "\n",
    "larger_than_average = words | 'larger' >> beam.Filter(lambda w, avg: w > avg, avg_word_length)\n",
    "# side input is a PCollectionView\n",
    "larger_than_average = words | 'larger' >> beam.FlatMap(filter_using_length, lower_bound=pvalue.AsSingleton(avg_word_length))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Every PCollection is processed within window**\n",
    "\n",
    "* the default window is global window,it starts when the data is input and ends when the last element in window  is processed\n",
    "* In Bounded Pcollections, commonly the elements are all marked as occuring at the same time, Example : TextIO does this so the global window basically ignores the timestamps \n",
    "* the global window is not useful for unbounded Pcollections, because the window never closes\n",
    "\n",
    "setting a single global window for a PCollection\n",
    "```python\n",
    "from apache_beam import window\n",
    "session_windowed_items =  items | 'window' >> beam.WindowInto(window.GlobalWindows())\n",
    "```\n",
    "\n",
    "Time-based windows can be useful for processing time-series data\n",
    "1- you may have to prepare the date-timestamp ,in this example the dts of the data (log writing time) becomes\n",
    " the element time . now the elements have different timestamps from one another\n",
    "2- using time based windowing the data is processed in groups . in  the example, each group gets its own average\n",
    "3- there are different kinds if windowing (fixed, sliding, session)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using windowing with Batch (group by time)\n",
    "```python\n",
    "lines = p | beam.io.ReadFromText('access.log')\n",
    "windowed_counts = (lines\n",
    "                   | 'Timestamp' >> beam.Map(beam.window.TimestampedValue(x,extract_timestamp(x)))\n",
    "                   | 'Window' >> beam.WindowInto(beam.window.SlidingWindows(60,30))\n",
    "                   | 'Count' >> (beam.combineGlobally(beam.combiners.CountCombineFn()).without_defaults())\n",
    "                   )\n",
    "windowed_counts = windowed_counts | beam.ParDo(PrintWindowFn())\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataflow template enable rapid deployment of standard jobs types\n",
    "dataflow template support non developer users to run jobs\n",
    "execute templates using cloud console , gcloud command line tool or rest api\n",
    "```shell\n",
    "gcloud dataflow jobs run myjob --gcs-location gs://dataflow-templates/latest/Word_Count inputTopic=projects/myproject/topics/mytopic outputTable=project:dataset.table\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
