{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Apache Spark jobs on Dataproc\n",
    "\n",
    "You are migrating an existing Spark workload to Cloud Dataproc \n",
    "and then progressively modifying the Spark code to make use of \n",
    "GCP native features and services. <br>\n",
    "Activate Google Cloud Shell <br>\n",
    "gcloud auth list <br>\n",
    "gcloud config list project <br>\n",
    "Check project permissions\n",
    "Configure and start a Cloud Dataproc\n",
    "cluster <br>\n",
    "1.In the GCP Console, on the Navigation menu, in the Big \n",
    "Data section, click Dataproc. <br>\n",
    "2.Click Create Cluster. <br>\n",
    "3.Enter sparktodp for Cluster Name. <br>\n",
    "4.In the Versioning section, click Change and select 2.0 (Debian 10, \n",
    "Hadoop 3.2, Spark 3.1). <br>\n",
    "This version includes Python3 which is required for the sample \n",
    "code used in this lab.<br>\n",
    "\n",
    "![imageVersion_dataproc](Media/imageVersion_dataproc.png)\n",
    "\n",
    "5.Click Select. <br>\n",
    "6.In the Components > Component gateway section, \n",
    "select Enable component gateway. <br>\n",
    "7.Under Optional components, Select Jupyter\n",
    "Notebook. <br>\n",
    "8.Click Create <br>\n",
    "\n",
    "![createCluster_dataproc](Media/createCluster_dataproc.png)\n",
    "\n",
    "**The cluster should start in a couple of \n",
    "minutes. You can proceed to the next \n",
    "step without waiting for the Cloud \n",
    "Dataproc Cluster to fully deploy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting(get repository, specify dataproc storage ,copy notebook to jupter working folder)\n",
    "Clone the source repository for the lab\n",
    "In the Cloud Shell you clone the Git repository for the lab and copy the required notebook files to the Cloud \n",
    "Storage bucket used by Cloud Dataproc as the home directory for Jupyter notebooks.\n",
    "1.To clone the Git repository for the lab enter the following command in Cloud Shell:\n",
    "```shell\n",
    "git -C ~ clone https://github.com/GoogleCloudPlatform/training-data-analyst \n",
    "```\n",
    "2. To locate the default Cloud Storage bucket used by Cloud Dataproc enter the following command in Cloud \n",
    "Shell: <br>\n",
    "```shell\n",
    "export DP_STORAGE=\"gs://$(gcloud dataproc clusters describe sparktodp --region=us-central1 --format=json | \n",
    "jq -r '.config.configBucket’)” \n",
    "```\n",
    "3. To copy the sample notebooks into the Jupyter working folder enter the following command in Cloud Shell: <br>\n",
    "```shell\n",
    "gsutil -m cp ~/training-data-analyst/quests/sparktobq/*.ipynb $DP_STORAGE/notebooks/jupyter <br>\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log in to the Jupyter Notebook\n",
    "As soon as the cluster has fully started up you can connect to the Web interfaces. \n",
    "Click the refresh button to check as it may be deployed fully by the time you reach \n",
    "this stage. <br>\n",
    "1.On the Dataproc Clusters page wait for the cluster to finish starting and then click \n",
    "the name of your cluster to open the Cluster details page. <br>\n",
    "2.Click Web Interfaces.<br>\n",
    "3.Click the Jupyter link to open a new Jupyter tab in your browser. <br>\n",
    "This opens the Jupyter home page. Here you can see the contents of \n",
    "the /notebooks/jupyter directory in Cloud Storage that now includes the sample \n",
    "Jupyter notebooks used in this lab. <br>\n",
    "4.Under the Files tab, click the GCS folder and then click 01_spark.ipynb notebook to \n",
    "open it. <br>\n",
    "5.Click Cell and then Run All to run all of the cells in the notebook. <br>\n",
    "6.Page back up to the top of the notebook and follow as the notebook completes \n",
    "runs each cell and outputs the results below them. <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code cell fetches the source data file, which is an extract from the KDD Cup competition from the \n",
    "Knowledge, Discovery, and Data (KDD) conference in 1999. The data relates to computer intrusion \n",
    "detection events.\n",
    "```shell\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-\n",
    "mld/kddcup.data_10_percent.gz\n",
    "```\n",
    "In the second code cell, the source data is copied to the default (local) Hadoop file system.\n",
    "```shell\n",
    "!hadoop fs -put kddcup* /\n",
    "```\n",
    "In the third code cell, the command lists contents of the default directory in the cluster's HDFS file system.\n",
    "```shell\n",
    "!hadoop fs -ls /\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook(Reading in data)\n",
    "The data are gzipped CSV files. In Spark, these can be read directly using the textFile method and \n",
    "then parsed by splitting each row on commas.\n",
    "The Python Spark code starts in cell In[4]. In this cell Spark SQL is initialized and Spark is used to \n",
    "read in the source data as text and then returns the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "data_file = \"hdfs:///kddcup.data_10_percent.gz\"\n",
    "raw_rdd = sc.textFile(data_file).cache()\n",
    "raw_rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean data\n",
    "csv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\n",
    "parsed_rdd = csv_rdd.map(lambda r: Row(\n",
    "duration=int(r[0]),\n",
    "protocol_type=r[1],\n",
    "service=r[2],\n",
    "flag=r[3],\n",
    "src_bytes=int(r[4]),\n",
    "dst_bytes=int(r[5]),\n",
    "wrong_fragment=int(r[7]),\n",
    "urgent=int(r[8]),\n",
    "hot=int(r[9]),\n",
    "num_failed_logins=int(r[10]),\n",
    "num_compromised=int(r[12]),\n",
    "su_attempted=r[14],\n",
    "num_root=int(r[15]),\n",
    "num_file_creations=int(r[16]),\n",
    "label=r[-1]\n",
    ")\n",
    ")\n",
    "parsed_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## query data\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(parsed_rdd)\n",
    "connections_by_protocol = \n",
    "df.groupBy('protocol_type').count().orderBy('count', \n",
    "ascending=False)\n",
    "connections_by_protocol.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"connections\")\n",
    "attack_stats = sqlContext.sql(\"\"\"\n",
    "SELECT\n",
    "protocol_type,\n",
    "CASE label\n",
    "WHEN 'normal.' THEN 'no attack'\n",
    "ELSE 'attack'\n",
    "END AS state,\n",
    "COUNT(*) as total_freq,\n",
    "ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
    "ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
    "df.registerTempTable(\"connections\")\n",
    "attack_stats = sqlContext.sql(\"\"\"\n",
    "SELECT\n",
    "protocol_type,\n",
    "CASE label\n",
    "WHEN 'normal.' THEN 'no attack'\n",
    "ELSE 'attack'\n",
    "END AS state,\n",
    "COUNT(*) as total_freq,\n",
    "ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
    "ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
    "ROUND(AVG(duration), 2) as mean_duration,\n",
    "SUM(num_failed_logins) as total_failed_logins,\n",
    "SUM(num_compromised) as total_compromised,\n",
    "SUM(num_file_creations) as total_file_creations,\n",
    "SUM(su_attempted) as total_root_attempts,\n",
    "SUM(num_root) as total_root_acceses\n",
    "FROM connections\n",
    "GROUP BY protocol_type, state\n",
    "ORDER BY 3 DESC\n",
    "\"\"\")\n",
    "attack_stats.show() ROUND(AVG(duration), 2) as \n",
    "mean_duration,\n",
    "SUM(num_failed_logins) as total_failed_logins,\n",
    "SUM(num_compromised) as total_compromised,\n",
    "SUM(num_file_creations) as total_file_creations,\n",
    "SUM(su_attempted) as total_root_attempts,\n",
    "SUM(num_root) as total_root_acceses\n",
    "FROM connections\n",
    "GROUP BY protocol_type, state\n",
    "ORDER BY 3 DESC\n",
    "\"\"\")\n",
    "attack_stats.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook(SparkSQL to Dataframe, viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ax = attack_stats.toPandas().plot.bar(x='protocol_type', \n",
    "subplots=True, figsize=(10,25))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![viz](Media/viz.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud storage instead of HDFS(copy data to new storage bucket)\n",
    "\n",
    "Modify Spark jobs to use Cloud Storage instead of HDFS\n",
    "Taking this original 'Lift & Shift' sample notebook you will now create a copy that decouples the storage requirements \n",
    "for the job from the compute requirements. In this case, all you have to do is replace the Hadoop file system calls with \n",
    "Cloud Storage calls by replacing hdfs:// storage references with gs:// references in the code and adjusting folder names \n",
    "as necessary.\n",
    "You start by using the cloud shell to place a copy of the source data in a new Cloud Storage bucket.\n",
    "1.In the Cloud Shell create a new storage bucket for your source data.\n",
    "```shell\n",
    "export PROJECT_ID=$(gcloud info --format='value(config.project)')\n",
    "gsutil mb gs://$PROJECT_ID\n",
    "```\n",
    "2. In the Cloud Shell copy the source data into the bucket.\n",
    "```shell\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data_10_percent.gz\n",
    "gsutil cp kddcup.data_10_percent.gz gs://$PROJECT_ID/\n",
    "```\n",
    "3.Switch back to the 01_spark Jupyter Notebook tab and Make a Copy with name De-couple-storage \n",
    "and close first one ,delete first three cells because we work with google cloud storage .\n",
    "4 .replace code in cell 4 with the following code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "gcs_bucket='[Your-Bucket-Name]'\n",
    "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "data_file = \"gs://\"+gcs_bucket+\"//kddcup.data_10_percent.gz\"\n",
    "raw_rdd = sc.textFile(data_file).cache()\n",
    "raw_rdd.take(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deploy Spark Jobs(Optimize Spark jobs to run on Job specific clusters)**\n",
    "\n",
    "3.Switch back to the 01_spark Jupyter Notebook tab and Make a Copy with name PySpark-analysis-file \n",
    "and close first one ,.\n",
    "4 .insert cell above with following code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile spark_analysis.py\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--bucket\", help=\"bucket for input and output\")\n",
    "args = parser.parse_args()\n",
    "BUCKET = args.bucket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **%%writefile **spark_analysis.py Jupyter magic command creates a new output file to contain your standalone python \n",
    "script. You will add a variation of this to the remaining cells to append the contents of each cell to the standalone script \n",
    "file.\n",
    "\n",
    "This code also imports the matplotlib module and explicitly sets the default plotting backend via matplotlib.use('agg') so that the \n",
    "plotting code runs outside of a Jupyter notebook. <br>\n",
    "10.For the remaining cells insert %%writefile -a spark_analysis.py at the start of each Python code cell. These are the five cells \n",
    "labelled In [x]. <br>\n",
    "%%writefile -a spark_analysis.py\n",
    "For example the next cell should now look as follows.\n",
    "```Python\n",
    "%%writefile -a spark_analysis.py\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "data_file = \"gs://{}/kddcup.data_10_percent.gz\".format(BUCKET)\n",
    "raw_rdd = sc.textFile(data_file).cache()\n",
    "#raw_rdd.take(5)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.Repeat this step, inserting %%writefile -a spark_analysis.py at the start of each code cell until you reach the end. <br>\n",
    "12.In the last cell, where the Pandas bar chart is plotted remove the %matplotlib inline magic command. <br>\n",
    "Note: You must remove this inline matplotlib Jupyter magic directive or your script will fail when you run it.\n",
    "13.Make sure you have selected the last code cell in the notebook then, in the menu bar, click Insert and \n",
    "select Insert Cell Below.\n",
    "14.Paste the following code into the new cell.\n",
    "```Python\n",
    "%%writefile -a spark_analysis.py\n",
    "ax[0].get_figure().savefig('report.png’);\n",
    "```\n",
    "15.Add another new cell at the end of the notebook and paste in the following:\n",
    "```Python\n",
    "%%writefile -a spark_analysis.py\n",
    "import google.cloud.storage as gcs\n",
    "bucket = gcs.Client().get_bucket(BUCKET)\n",
    "for blob in bucket.list_blobs(prefix='sparktodp/'):\n",
    "blob.delete()\n",
    "bucket.blob('sparktodp/report.png').upload_from_filename('report.png’)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Automation\n",
    "\n",
    "You now test that the PySpark code runs successfully as a file by calling the local copy from inside the notebook, \n",
    "passing in a parameter to identify the storage bucket you created earlier that stores the input data for this job. The \n",
    "same bucket will be used to store the report data files produced by the script. <br>\n",
    "1.In the PySpark-analysis-file notebook add a new cell at the end of the notebook and paste in the following:<br>\n",
    "```shell\n",
    "BUCKET_list = !gcloud info --format='value(config.project)'\n",
    "BUCKET=BUCKET_list[0]\n",
    "print('Writing to {}'.format(BUCKET))\n",
    "!/opt/conda/miniconda3/bin/python spark_analysis.py --bucket=$BUCKET\n",
    "```\n",
    "This code assumes that you have followed the earlier instructions and created a Cloud Storage Bucket \n",
    "using your lab Project ID as the Storage Bucket name. If you used a different name modify this code to \n",
    "set the BUCKET variable to the name you used.\n",
    "2. Add a new cell at the end of the notebook and paste in the following:\n",
    "\n",
    "```shell\n",
    "!gsutil ls gs://$BUCKET/sparktodp/**\n",
    "```\n",
    "This lists the script output files that have been saved to your Cloud Storage bucket.\n",
    "\n",
    "3.To save a copy of the Python file to persistent storage, add a new cell and paste in the following:\n",
    "```shell\n",
    "!gsutil cp spark_analysis.py gs://$BUCKET/sparktodp/spark_analysis.py\n",
    "```\n",
    "4.Run All\n",
    "![test_automation](Media/test_automation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Automation(Run the Analysis Job from Cloud Shell)\n",
    "1.Switch back to your Cloud Shell and copy the Python script from Cloud Storage so you can run it as a Cloud \n",
    "Dataproc Job.\n",
    "```shell\n",
    "gsutil cp gs://$PROJECT_ID/sparktodp/spark_analysis.py spark_analysis.py\n",
    "```\n",
    "2. Create a launch script.\n",
    "\n",
    "```shell\n",
    "nano submit_onejob.sh\n",
    "```\n",
    "1. Paste the following into the script:\n",
    "\n",
    "```shell\n",
    "#!/bin/bash\n",
    "gcloud dataproc jobs submit pyspark \\\n",
    "--cluster sparktodp \\\n",
    "--region us-central1 \\\n",
    "spark_analysis.py \\\n",
    "-- --bucket=$1\n",
    "```\n",
    "4.Press CTRL+X then Y and Enter key to exit and save.\n",
    "5.Make the script executable:\n",
    "6.Launch the PySpark Analysis job:\n",
    "```powershell\n",
    "./submit_onejob.sh $PROJECT_ID\n",
    "```\n",
    "7.In the Cloud Console tab navigate to \n",
    "the Dataproc > Clusters page if it is not already open. <br>\n",
    "8.Click Jobs , wait until job finished. <br>\n",
    "10.Navigate to your storage bucket and note that the \n",
    "output report, /sparktodp/report.png has an updated timestamp indicating that the stand-alone job has completed \n",
    "successfully. <br>\n",
    "The storage bucket used by this Job for input and output \n",
    "data storage is the bucket that is used just the Project ID \n",
    "as the name. <br>\n",
    "11.Navigate back to the Dataproc > Clusters page. <br>\n",
    "12.Select the sparktodp cluster and click Delete. You don't \n",
    "need it any more. <br>\n",
    "13.Click CONFIRM. <br>\n",
    "14.Close the Jupyter tabs in your browser. <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### appendix\n",
    "#### Check project permissions\n",
    "Before you begin your work on Google Cloud, you need to ensure that your project has the correct permissions \n",
    "within Identity and Access Management (IAM).\n",
    "1.In the Google Cloud console, on the Navigation menu ( ), click IAM & Admin > IAM.\n",
    "2.Confirm that the default compute Service Account {project-number}-compute@developer.gserviceaccount.com is \n",
    "present and has the editor role assigned. The account prefix is the project number, which you can find on Navigation \n",
    "menu > Home\n",
    "![permissions](Media/permissions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
