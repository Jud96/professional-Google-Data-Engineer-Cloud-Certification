{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cloud functions \n",
    "cloud functions are a serverless way to run code in the cloud based on event_driven (http trigger , timer trigger) <br>\n",
    "\n",
    "![cloud functions](../Media/computing/cloud_functions.png)\n",
    "\n",
    "our task today is \n",
    "schedlue a function to run at a specific time <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create  cloud function \n",
    "![create cloud function](cloudFunction_createFunction.png)\n",
    "| attribute              | value               |\n",
    "| ---------------------- | ------------------- |\n",
    "| vicinity               | 1st ge              |\n",
    "| function name          | xxxx                |\n",
    "| region                 | xxxx                |\n",
    "| trigger type           | http                |\n",
    "| authentification       | no authentification |\n",
    "| duration allocated mem |  60sec / 256 MB     |\n",
    "|autoscale               | 1 instance          |\n",
    "\n",
    "![create cloud function code](cloudFunction_createFunctioncode.png)\n",
    "\n",
    "~~~python\n",
    "def hello_world(request):\n",
    "    \"\"\"Responds to any HTTP request.\n",
    "    Args:\n",
    "        request (flask.Request): HTTP request object.\n",
    "    Returns:\n",
    "        The response text or any set of values that can be turned into a\n",
    "        Response object using\n",
    "        `make_response <http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response>`.\n",
    "    \"\"\"\n",
    "    request_json = request.get_json()\n",
    "    if request.args and 'message' in request.args:\n",
    "        return request.args.get('message')\n",
    "    elif request_json and 'message' in request_json:\n",
    "        return request_json['message']\n",
    "    else:\n",
    "        return f'Hello World!'\n",
    "~~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Runtime : Python 3.8  <br>\n",
    "Entry point : hello_world <br>\n",
    "source code <br>\n",
    "requirements.txt : add your packages here <br>\n",
    "![cloud function trigger](cloudFunction_trigger.png)\n",
    "\n",
    "Trigger : HTTP (you can invoke it by http request) <br>\n",
    "![cloud function test](cloudFunction_test.png)\n",
    "\n",
    "test tab : you can test your function here by invoking http request <br>\n",
    "go to trigger tab and copy the url <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cloud scheduler\n",
    "create a cloud scheduler to invoke your function <br>\n",
    "1- go to cloud scheduler api and enable it <br>\n",
    "2- go to cloud scheduler and create a job <br>\n",
    "\n",
    " ![cloud scheduler create job](cloudScheduler_createJob.png)\n",
    "\n",
    " ![cloud scheduler create job](cloudScheduler_configexecution.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scenario\n",
    "customer upload xlsx files about sales to blob storage. he want to transfer this data to postgres db to further analysis. <br>\n",
    "![cloud function demo architecture](functionapp.png)\n",
    "\n",
    "### logic app \n",
    "1. access to data sources/sink (connectors)\n",
    "2. access to new files that not in sink based on max date in db \n",
    "3. merge data to one dataframe\n",
    "4. clean data \n",
    "5. insert data to db (if there is new data)\n",
    "6. update max date in db\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "\n",
    "import azure.functions as func\n",
    "import logging\n",
    "from datetime import datetime,timedelta, date, timezone\n",
    "# import configparser \n",
    "import psycopg2\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "import psycopg2.extras as extras\n",
    "\n",
    "\n",
    "def execute_values(conn, df, table):\n",
    "  \n",
    "    tuples = [tuple(x) for x in df.to_numpy()]\n",
    "  \n",
    "    cols = ','.join(list(df.columns))\n",
    "    # SQL query to execute\n",
    "    query = \"INSERT INTO %s(%s) VALUES %%s\" % (table, cols)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        extras.execute_values(cursor, query, tuples)\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    print(\"the dataframe is inserted\")\n",
    "    cursor.close()\n",
    "\n",
    "def lambda_handler():\n",
    "    # TODO implement\n",
    "    # config = configparser.ConfigParser()\n",
    "    # config.read('config.cfg')\n",
    "    # KEY = config.get('Blob','KEY')\n",
    "    # accounturl = config.get('Blob','accounturl')\n",
    "    KEY = 'xxxxx'\n",
    "    accounturl = 'xxxxx' \n",
    "    # accsss to blob storage\n",
    "    blob_service_client = BlobServiceClient(account_url=accounturl, credential=KEY)\n",
    "\n",
    "    # see blobs in container\n",
    "    filelist = blob_service_client.get_container_client('shipmentlist').list_blobs()\n",
    "\n",
    "# extract file name with specific regular expression that include versandliste_ digits and end with .xls\n",
    "\n",
    "    list2 = []  # include only files with specific regular expression (required files)\n",
    "    for i in filelist:\n",
    "    # begin with versandliste_ numbers or spaces and end with .xls\n",
    "        if re.match(r'Versandliste_[0-9 _]+.xls', i.name):\n",
    "            list2.append(i.name)\n",
    "#extract date from list of strings and convert to datetime format\n",
    "    list4 = []\n",
    "    for i in list2:\n",
    "        try:\n",
    "            list4.append(datetime.strptime(i.split('_')[1], '%Y %m %d'))\n",
    "        except:\n",
    "            list4.append(datetime.strptime(i.split('_')[1], '%Y%m%d'))\n",
    "\n",
    "    # connect to database\n",
    "    # host = config.get('Postgres','host')\n",
    "    # port = config.get('Postgres','port')\n",
    "    # database = config.get('Postgres','database')\n",
    "    # username = config.get('Postgres','username')\n",
    "    # password = config.get('Postgres','password')\n",
    "    host = 'xxx'\n",
    "    port =  5432\n",
    "    database = 'xxx'\n",
    "    username = 'xxxx'\n",
    "    password = 'xxxx'\n",
    "\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "        database=database, user=username, password=password, host=host, port= port\n",
    "    )\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # fetch last date from our database\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT value \n",
    "        FROM \"Keypairs\" u\n",
    "        WHERE u.key = %s;\n",
    "        \"\"\",\n",
    "        ['last_date',]\n",
    "    )\n",
    "    res = cursor.fetchall()\n",
    "    last_date = res[0][0]\n",
    "    last_date = datetime.strptime(last_date,'%Y-%m-%d')\n",
    "    logging.info('last_date: %s', last_date)\n",
    "    # find new files \n",
    "    requried_files = []\n",
    "    for i in range(1,len(list4)):\n",
    "        if list4[i] > last_date:\n",
    "            requried_files.append(list2[i])\n",
    "\n",
    "    if(len(requried_files) == 0):\n",
    "        print(\"No new files\")\n",
    "        return 0\n",
    "    else:\n",
    "        df = pd.read_excel(io.BytesIO(blob_service_client.get_blob_client('shipmentlist',requried_files[0]).download_blob().readall()),skiprows=5)\n",
    "        df = df.dropna(subset=['ID'])\n",
    "        df['trackinglinks'] = df[df.columns[df.columns.str.match(r'Trackinglink\\d+')]].apply(\n",
    "            lambda x: ','.join(x.dropna().astype(str)),\n",
    "            axis=1\n",
    "        )\n",
    "        # drop columns that contain trackinglinks\n",
    "        df = df.loc[:,df.columns.str.match(r'Trackinglink\\d+') == False]\n",
    "\n",
    "    # join files if there are more than one file\n",
    "    for i in range(1,len(requried_files)):\n",
    "        df2 = pd.read_excel(io.BytesIO(blob_service_client.get_blob_client('shipmentlist',requried_files[i]).download_blob().readall()),skiprows=5)\n",
    "        df2 = df2.dropna(subset=['ID'])\n",
    "        df2['trackinglinks'] = df2[df2.columns[df2.columns.str.match(r'Trackinglink\\d+')]].apply(\n",
    "            lambda x: ','.join(x.dropna().astype(str)),\n",
    "            axis=1\n",
    "        )\n",
    "        # drop columns that contain trackinglinks\n",
    "        df2 = df2.loc[:,df2.columns.str.match(r'Trackinglink\\d+') == False]\n",
    "        df = pd.concat([df,df2],ignore_index=True)\n",
    "\n",
    "    # clean data \n",
    "    df.columns= ['ID', 'Artikelnummer', 'Größe', 'Charge', 'PorticAartikelNummer','Gewicht', 'Höhe', 'Breite', 'Tiefe', 'Volumen', 'Lager', 'ArtikelBezeichnung', 'VKE', 'Menge', 'Einzel_VK_Netto', 'Gesamt_VK_Netto', 'LieferscheinNummer', 'RS_Nr', 'LieferscheinDatum', 'Waagedatum', 'Packanteil', 'AuftragsNummer', 'Bestelldatum', 'AngebotsBezeichnung', 'EMail_Besteller', 'KundennummerRechnung', 'KundennummerLieferung', 'Lieferempfänger', 'Lieferstr', \\\n",
    "                'Liefer_PLZ', 'Lieferort', 'Lieferland', 'NichtEU', 'Bestellweg','trackinglinks']\n",
    "\n",
    "    # casting varialbes to correct data type\n",
    "    df['Waagedatum'] = pd.to_datetime(df['Waagedatum'], format='%d.%m.%Y')\n",
    "    df['LieferscheinDatum'] = pd.to_datetime(df['LieferscheinDatum'], format='%d.%m.%Y')\n",
    "    df['Bestelldatum'] = pd.to_datetime(df['Bestelldatum'], format='%d.%m.%Y')\n",
    "    # conver Artikelnummer to integer\n",
    "    df['ID'] = df['ID'].astype(int)\n",
    "    df['KundennummerRechnung'] = df['KundennummerRechnung'].astype(int)\n",
    "    df['KundennummerLieferung'] = df['KundennummerLieferung'].astype(int)\n",
    "    df['LieferscheinNummer'] = df['LieferscheinNummer'].astype(int)\n",
    "    df['Menge'] = df['Menge'].astype(int)\n",
    "    df['VKE'] = df['VKE'].astype(int)\n",
    "    df = df.drop('ID',axis=1) # drop ID column because it is not required,serial in our data base\n",
    "    execute_values(conn, df, 'versandliste')\n",
    "    # df_fact = df[['Artikelnummer','ArtikelBezeichnung','VKE','Menge','Einzel_VK_Netto','Gesamt_VK_Netto','LieferscheinDatum','KundennummerRechnung','KundennummerLieferung','Lieferempfänger','Lieferstr','Liefer_PLZ','Lieferort','Lieferland','NichtEU']]\n",
    "    # execute_values(conn, df_fact, 'versandliste_fact')\n",
    "    # find max date in new files \n",
    "    max_date = df['Waagedatum'].max()\n",
    "    max_date = max_date.strftime('%Y-%m-%d')\n",
    "    cursor.execute(\n",
    "    \"\"\"\n",
    "    Update \"Keypairs\" u\n",
    "    set value = %s\n",
    "    WHERE u.key = 'last_date';\n",
    "    \"\"\",\n",
    "    [max_date]\n",
    "    )\n",
    "def hello_world(request):\n",
    "   \n",
    "    print('begin work')\n",
    "    lambda_handler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secret Manager API \n",
    "Secret Manager lets you store, manage, and secure access to your application secrets. Secret Manager provides a central place and single source of truth to manage, access, and audit secrets across Google Cloud.\n",
    "\n",
    "what secret Manager feature ?\n",
    "1. store secrets\n",
    "2. manage secrets\n",
    "3. audit secrets (who access secrets,logs,alerts)\n",
    "4. rotate secrets\n",
    "5. access secrets\n",
    "6. versioning secrets\n",
    "7. replication secrets\n",
    "8. cloud IAM integration\n",
    "9. Encryption at rest\n",
    "\n",
    "cloud function should have access to secret manager to access secrets so that go to AIM and add secret manager role to your cloud function service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import secretmanager\n",
    "def access_secret_version(project_id, secret_id, version_id):\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "    return response.payload.data.decode(\"UTF-8\")\n",
    "host = access_secret_version('norse-sector-386912','host','1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
