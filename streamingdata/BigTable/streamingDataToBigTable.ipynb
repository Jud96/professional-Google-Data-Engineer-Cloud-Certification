{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "✓ Open the SSH terminal and connect to the training VM <br>\n",
    "Compute Engine > VM instances> training-vm > Connect. <br>\n",
    "✓ in VM terminal write code ls /training <br>\n",
    "✓ Download Code Repository <br>\n",
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst <br>\n",
    "✓ in VM terminal Set environment variable <br> \n",
    "source /training/project_env.sh <br>\n",
    "✓ Prepare HBase quickstart files <br>\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./install_quickstart.sh <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate traffic sensor data into Pub/Sub\n",
    "✓ in VM terminal run sensor magic code <br>\n",
    "/training/sensor_magic.sh <br>\n",
    "✓ upper right corner of the training-vm SSH terminal New Connection to training-vm <br>\n",
    "✓ Set environment variables in second VM terminal <br>\n",
    "source /training/project_env.sh <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Dataflow Pipeline\n",
    "✓ In the second training-vm <br>\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego <br>\n",
    "nano run_oncloud.sh <br>\n",
    "CTRL+X # interrupt <br>\n",
    "✓ Run the following script to create the Bigtable instance <br>\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego <br>\n",
    "./create_cbt.sh <br>\n",
    "✓ Run the Dataflow pipeline to read from PubSub and write into Cloud Bigtable <br>\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego <br>\n",
    "./run_oncloud.sh $DEVSHELL_PROJECT_ID $BUCKET CurrentConditions --bigtable <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the pipeline , Query Bigtable data\n",
    "\n",
    "✓ Navigation >> Dataflow >> graph >> write:cbt step, Review the Bigtable Options in Step summary <br>\n",
    "✓ second training-vm <br>\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego/quickstart <br>\n",
    "./quickstart.sh <br>\n",
    "✓ script completes you are in HBase shell prompt <br>\n",
    "scan 'current_conditions', {'LIMIT' => 2} # query to retrieve 2 rows from your Bigtable  <br>\n",
    "“each row is broken into column, timestamp, value combinations” <br>\n",
    "✓ This time look only at the <br> lane: speed column, limit to 10 rows, and specify rowid patterns for start \n",
    "and end rows to scan over\n",
    "scan 'current_conditions', {'LIMIT' => 10, STARTROW => '15#S#1', ENDROW => '15#S#999', COLUMN \n",
    "=> 'lane:speed’} <br>\n",
    "✓ Exit shell <br>\n",
    "quit\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup (release sources )\n",
    "\n",
    "1.In the second training-vm SSH terminal, run the following script to delete your Bigtable instance.\n",
    "cd ~/training-data-analyst/courses/streaming/process/sandiego\n",
    "./delete_cbt.sh\n",
    "If prompted to confirm, enter Y.\n",
    "2.On your Dataflow page in your Cloud Console, click on the pipeline job name.\n",
    "3.Click Stop on the top menu bar. Select Cancel, and then click Stop Job.\n",
    "4.Go back to the first SSH terminal with the publisher, and enter Ctrl+C to stop it.\n",
    "5.In the BigQuery console, click on the three dots next to the demos dataset, and click Delete.\n",
    "6.Type delete and then click Delete."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theory notes:\n",
    " big table is a no sql database, it is a columnar database, it is a key value store, it is a wide column store,for large datasets (petabytes) it is a high throughput and low latency \n",
    "\n",
    " how to choose between big table and big query?\n",
    " if acceptable time to insight in miliseconds then bigtable is the choice, if acceptable time to insight in seconds then big query is the choice\n",
    "\n",
    " in big table (cluster based service high throughput scales linearly optimized for fast store NoSQL) 100,000 queries persecond (10 nodes at 6ms latency)\n",
    "\n",
    " bigquery (serveless optimized for fast query SQL)\n",
    " 100,000 rows per second (streaming performance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigtable design idea is \"simplify for speed\"\n",
    "Row Key is the only index\n",
    "\n",
    "speed depend on your data and row key \n",
    "\n",
    "row key here is orgin#arrival"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bigtable_schema](../Media/bigtable_schema.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use reverse timestamps when your most common query is for the latest values \n",
    "Query : current arrival delay for flights from Atlanta\n",
    "// key is ORIGIN#arrival#REVTS\n",
    "string key = info.getOrigin() + \"#arrival\" + \"#\" + (Long.MAX_VALUE - ts.getMillis()); // reverse timestamp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![changeDataInBigTable](../Media/changeDataInBigTable.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizing data organization in big table for performance\n",
    "* group related data together for moe efficient reads\n",
    "* distribute data evenly to avoid hotspots (efficient writes)\n",
    "* place identical values in same row or adjoining rows for more efficient compression\n",
    "\n",
    "Bigtable self_improves by learning access patterns and optimizing data layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spotify_bigtable](../Media/spotify_bigtable.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimizing_bigtable](../Media/optimizing_bigtable.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![replication_bigtable](../Media/replication_bigtable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
