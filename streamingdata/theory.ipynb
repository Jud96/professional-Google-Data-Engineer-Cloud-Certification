{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming is data processing for unbounded data sets\n",
    "\n",
    "Bounded data (Batch) is data that has a defined start and end\n",
    "Unbounded data (Streaming) is data that is continuous and has no defined start or end\n",
    "\n",
    "Stream analytics has many applications \n",
    "**Data integration**(10 sec - 10 min )\n",
    "* data warehouses become real time\n",
    "* take load off source databases with change data capture (CDC)\n",
    "* microservices require databases and caches\n",
    "\n",
    "**Online decisions** (10 ms - 10 sec)\n",
    "* fraud detection\n",
    "* real time recommendations\n",
    "* gaming event \n",
    "* financial trading\n",
    "\n",
    "Google Cloud Platform (GCP) has many services for stream analytics\n",
    "* Pub/Sub : messaging service (changing and variable volumes of data)\n",
    "* Dataflow : stream processing engine (processing data)\n",
    "* bigquery : data warehouse (storing data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![streamAnalytics](Media/streamAnalytics.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dataflow offers the following that makes it easy to create resilient streaming pipelines when working with unbounded data:\n",
    "Ability to flexibly reason about time\n",
    "Controls to ensure correctness\n",
    "\n",
    "Pub/Sub : Global message queue\n",
    "Dataflow : Controls to handle late-arriving and out-of-order data\n",
    "BigQuery : Query data as it arrives from streaming pipelines\n",
    "Bigtable  :Latency in the order of milliseconds when querying against overwhelming volume\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pubsub](Media/pub_sub_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the pub/sub events on the message attributes \n",
    "Configure via the cloud console, gcloud commond line tools or pub/sub api\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pub_sub_patterns](Media/pub_sub_patterns.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pushAndPullDelivery](Media/pushAndPullDelivery.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![subscribers_work_method](Media/subscribers_work_method.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# create a topic and publish a message\n",
    "gcloud pubsub topics create sandiego\n",
    "gcloud pubsub topics publish sandiego --message \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from google.cloud import pubsub_v1\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_name = 'projects/{project_id}/topics/{topic}'.format(\n",
    "    project_id=os.getenv('GOOGLE_CLOUD_PROJECT'),\n",
    "    topic='my-new-topic',  # Set this to something appropriate.\n",
    ")\n",
    "publisher.create_topic(topic_name)\n",
    "publisher.publish(topic_name, b'My first message!',author='bakro')# author is send attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from google.cloud import pubsub_v1\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "subscription_name = 'projects/{project_id}/subscriptions/{sub}'.format(\n",
    "    project_id=os.getenv('GOOGLE_CLOUD_PROJECT'),\n",
    "    sub='my-new-subscription',  # Set this to something appropriate.\n",
    ")\n",
    "subscriber.create_subscription(name =subscription_name,topic =topic_name)\n",
    "\n",
    "# pull method callbac function\n",
    "def callback(message):\n",
    "    print(message.data)\n",
    "    message.ack()\n",
    "subscriber.subscribe(subscription_name, callback=callback)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "gcloud pubsub subscriptions create mySubscription --topic sandiego mysub1\n",
    "gcloud pubsub subscriptions pull --auto-ack mySubscription\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from google.cloud import pubsub_v1\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "subscription_path = subscriber.subscription_path(project_id, subscription_name)\n",
    "\n",
    "NUM_MESSAGES = 2\n",
    "ACK_DEADLINE = 30\n",
    "SlEEP_TIME = 10\n",
    "response = subscriber.pull(subscription_path, max_messages=NUM_MESSAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub\n",
    "from google.cloud.pubsub import types\n",
    "# change batch settings\n",
    "client = pubsub.PublisherClient(batch_settings=types.BatchSettings(max_messages=500))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duplication will happen\n",
    "If messages have the same ordering key and are in the same region, you can enable message ordering. <br>\n",
    "• To receive the messages in order, set the message ordering property on the subscription you receive messages from using the Cloud Console, the gcloud command-line tool, or the Pub/Sub API. <br>\n",
    "• Receiving messages in order might increase latency. <br>\n",
    "\n",
    "![pubsub2](Media/pubsub_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pub/Sub lets you configure an exponential backoff policy for better flow control. <br>\n",
    "• The idea behind exponential backoff is to add progressively longer delays between retry attempts. <br>\n",
    "• To create a new subscription with an exponential backoff retry policy, run the gcloud pubsub create command or use the Cloud console. <br>\n",
    "\n",
    "summary <br>\n",
    "latency,out_of_order,duplication will happen  <br>\n",
    "pub/sub with dataflow:EXactly once,ordered processing <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pub/Sub  simplifies systems by removing the need for every component to speak to every component <br>\n",
    "Pub/Sub connects applications and services through a messaging infrastructure <br>\n",
    "Pub/Sub doesn’t guarantees that messages delivered are in the order they were received <br>\n",
    "\n",
    "Which of the following about Pub/Sub topics and subscriptions are true?\n",
    "1 or more publisher(s) can write to the same topic <br>\n",
    "1 or more subscriber(s) can request from the same subscription <br>\n",
    "vWhich of the following delivery methods is ideal for subscribers needing close to real time performance? <br>\n",
    "Push Delivery  <br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are challenges with processing unbounded data\n",
    "* scalability : streaming data is continuous and can be very large\n",
    "* fault tolerance : maintain fault tolerance despite increasing data volumes\n",
    "* Model : is it streaming or batch data?\n",
    "* Timing : what if data arrives late or out of order?\n",
    "\n",
    "how do you aggregate data in a stream?\n",
    "windowing : divide the stream into finite sets of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pubsub_dataflow](Media/pubsub_dataflow.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataflow2](Media/dataflow2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to modify date-timestamp\n",
    "```python\n",
    "unix_timestamp = extract_timestamp_from_log_entry(element)\n",
    "yield beam.window.TimestampedValue(element, unix_timestamp)\n",
    "```\n",
    "```java\n",
    "c.outputWithTimestamp(element, timestamp );\n",
    "```\n",
    "\n",
    "Dupplciation will happen : Exactly once processing with pub/sub and dataflow\n",
    "\n",
    "```shell\n",
    "# pub/sub publisher code\n",
    "msg.publish(event, myid=\"23tfkdjg\")\n",
    "```\n",
    "```java\n",
    "// dataflow pipeline code\n",
    "p.apply(PubsubIO.readStrings().fromTopic(t).idlLabel(\"myid\"))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![windows](Media/windows.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting time windows\n",
    "```python\n",
    "# fixed time windows\n",
    "from apache_beam import window\n",
    "fixed_windowed_items =( item | 'window'>> beam.WindowInto(window.FixedWindows(60)))\n",
    "```\n",
    "\n",
    "```python\n",
    "# fixed time windows\n",
    "from apache_beam import window\n",
    "sliding_windowed_items =( item | 'window'>> beam.WindowInto(window.SlidingWindows(60,30)))\n",
    "```\n",
    "\n",
    "```python\n",
    "from apache_beam import window\n",
    "session_windowed_items =( item | 'window'>> beam.WindowInto(window.Sessions(10*60)))\n",
    "```\n",
    "\n",
    "Remember : you can apply windows to batch data, although you may need to generate metadata date_timestamp on which windows operate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![window1](Media/window1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![window2](Media/window2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![window3](Media/window3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![watermark](Media/watermark.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![window_customtrigger](Media/window_customtrigger.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some example triggers\n",
    "```python\n",
    "pcollection | windowInto(sliding_window(60,5), # 60 second window, 5 second slide\n",
    "            trigger = AfterWatermark( # relative to the watermark\n",
    "                early=AfterProcessingTime(30), # fire 30 secs after pipeline commences\n",
    "                late=AfterProcessingTime(1) # and for every second thereafter\n",
    "            )\n",
    "            accumulationMode = AccumulationMode.ACCUMULATING)\n",
    "            allowed_lateness = Duration(seconds=2*24*60*60) # 2 days\n",
    "       \n",
    "```\n",
    "```python\n",
    "pcollection| windowInto(\n",
    "    FixedWindows(60), # 60 seconds\n",
    "    trigger = Repeatedly(\n",
    "        AfterAny(\n",
    "            AfterCount(10), # every 10 elements\n",
    "            AfterProcessingTime(30) # every 30 seconds\n",
    "        )),\n",
    "    accumulation_mode = AccumulationMode.DISCARDING) #the trigger should be with only new records\n",
    "```\n",
    "**you can allow late data past the watermark**\n",
    "```java\n",
    "Pcollection<string> items = ...;\n",
    "PCollection<string> windowed_items = items.apply(\n",
    "    Window.<string>into(FixedWindows.of(Duration.standardMinutes(1)))\n",
    "    .withAllowedLateness(Duration.standardMinutes(1))\n",
    "```\n",
    "\n",
    "```python\n",
    "pc = [Initail PCollection]\n",
    "pc | beam.WindowInto(window.FixedWindows(60),\n",
    "    trigger = trigger_fn,\n",
    "    accumulation_mode = accumulation_mode,\n",
    "    timestamp_combiner = timestamp_combiner,\n",
    "    allowed_lateness = duration(seconds=2*24*60*60)) # 2 days\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Accumulation_modes](Media/Accumulation_modes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
